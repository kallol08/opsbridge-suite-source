
# The EULA can be found at: https://www.microfocus.com/en-us/legal/software-licensing
# You must accept the Open Text EULA to deploy Opsbridge Suite.
acceptEula: false

persistence:
  # OPSB creates 4 PVCs using the defaultStorageClass.
  # This is one mechanism for changing the storageClass, the other is global.persistence.storageClasses.* found below.
  defaultStorageClass:

#[DO NOT CHANGE]
terminalFalse: false

global:
  # [REQUIRED] Externally accessible hostname/FQDN (Load balancer OR Master Node). Provide the hostname/FQDN in lower case letters. 
  externalAccessHost:
  # [REQUIRED] Externally accessible port (Load balancer OR Master Node). External Access Port along with External Access Host is used to access Opsbridge Suite.
  externalAccessPort:
  # [DO NOT CHANGE] If set to true, will inject the hosts into ingress, otherwise host will be empty.
  setFqdnInIngress: true
  
  #If the value of any of the global.rbac.* keys is false, then it is the responsibility of the customer to create service accounts, roles and cluster roles before starting the chart installation.
  rbac:
    serviceAccountCreate: true
    roleCreate: true

  # Sets the type of cert that ServiceMonitor objects should use as default CA
  vault:
    realmList: "RE:365"

  expose:
    enableALB: false           # set to true to enable alb 
    type:                      # For all the services to be exposed outside, default service type is of "Nodeport", with this parameter we can change service type to "LoadBalancer","ClusterIP" depending on need
    internalLoadBalancer:
      annotations: {}          # Annotation specified here will be applicable to the Load balancer that will be created during Opsbridge Installation; It specifies the annotations required for the Kubernetes Provider
      ip:                      # This parameter ensures that all the services of type LoadBalancer have same IP address assigned which is specified in this value
      sourceRanges: []         # This parameter specifies the allowed load balancer IP range
    ipConfig:
      ipFamilyPolicy: PreferDualStack   # This parameter specifies the ipFamilyPolicy that will be used by the k8s service


  # Every service here represents a functionality Opsbridge Suite offers. Users can enable one or multiple components.
  # reporting: Reporting component contains OPTIC Data Lake, BVD and Collection Service, if "true" reporting will be installed.
  # automaticEventCorrelation: Automatic Event Correlation component contains Automatic Event Correlation and OPTIC Data Lake.
  # stakeholderDashboard: Stakeholder dashboard contains BVD component.
  # obm: Operations Bridge Manager component.

  services:
# User must uncomment the capabilities they want to deploy and comment the ones they do no want to deploy
    automaticEventCorrelation:
#      deploy: true
      foo: bar
    stakeholderDashboard:
#      deploy: true
      foo: bar
    obm:
#      deploy: true
      deployRtsm: false # disable ucmdb within OBM subchart (temporary)
      foo: bar
    hyperscaleObservability:
#      deploy: true                            # This parameter deploys all HSO functionalities (such as AWS, Azure, K8s, Virtualization Event Collector). Uncomment this parameter to deploy the HSO capability with Optic DL.
#      deployLightHSO: true                    # This parameter only deploys HSO functionalities that do not use Optic DL (such as Virtualization Event Collector). Uncomment this parameter to deploy HSO without Optic DL. Note that enabling any other capability that requires Optic DL would still trigger Optic DL deployment.
      foo: bar
    agentlessMonitoring:
#      deploy: true
      foo: bar
    anomalyDetection:                           # Anomaly Detection is a technology preview capability. Enable this capability only if OBA is going to be installed and integrated with OPTIC DL to try a technology preview of the new Anomaly Detection Configurator and OPTIC DL Source Configurator user experience interfaces.
#      deploy: true
      foo: bar
    applicationMonitoring:                        # Application Monitoring provides admin UI for APM End User Management Configuration
#      deploy: true
      foo: bar
    opticReporting:
#      deploy: true
    opticDataLake:
#      deploy: false                            # This parameter decides whether or not Optic Data lake will be deployed. If this parameter is commented then optic Data Lake will be deployed, else provide parameters required to access Shared optic DL.

      pulsar:
        #deploy: false                          # This parameter decides whether or not Optic Data Lake Message Bus will be deployed. If this parameter is commented then optic Data Lake Message Bus will be deployed, else provide parameters required to access Shared optic DL Message Bus.
        externalPulsar:                         # This contains the parameters needed to access Optic DL Message Bus deployed in a different namespace
          connectUsingNamespace:                # Connect using cross-namespace communication within same cluster
            namespace:                          # Namespace of the providing deployment

      externalOpticDataLake:                    # This contains all the parameters needed to access Optic DL deployed in different namespace
        externalAccessHost:                     # External Access Host of the providing deployment
        externalAccessPort:                     # Ingress Controller Port of the providing deployment
        integrationUser:                        # IDM Integration User of the providing deployment
        integrationPasswordKey: OPTIC_DATALAKE_INTEGRATION_PASSWORD # [DO NOT CHANGE] IDM Integration User password of the providing deployment
        # User must use one of the below connection mechanisms if using Shared Optic Data Lake
        connectUsingNamespace:                  # Connect using cross-namespace communication within same cluster
          namespace:                            # Namespace of the providing deployment
        connectUsingServiceFQDN:                # Connect using service specific FQDNs. Use this Mechanism If you are using ALB on AWS.
          diReceiverHost:                       # Hostname of OPTIC Reporting Ingestion Service
          diReceiverPort:                       # (Service FQDN) Port of OPTIC Reporting Ingestion Service
          diAdminHost:                          # Hostname of OPTIC Reporting Administration Service
          diAdminPort:                          # (Service FQDN) Port of OPTIC Reporting Administration Service
          diDataAccessHost:                     # Hostname of OPTIC Reporting Data Access Service
          diDataAccessPort:                     # (Service FQDN) Port of OPTIC Reporting Data Access Service
          diPulsarHost:                         # Hostname of OPTIC Reporting Pulsar service
          diPulsarSslPort:                      # (Service FQDN) Port of OPTIC Reporting Pulsar SSL service
          diPulsarWebPort:                      # (Service FQDN) Port of OPTIC Reporting Pulsar Web service
          ingressControllerHost:                # Ingress Controller FQDN of Providing namespace: Used to connect to the IDM deployed in the providing namespace
          ingressControllerPort:                # Ingress Controller port of Providing namespace: Used to connect to the IDM deployed in the providing namespace
        connectUsingExternalAccessHost:         # Connect using External Access Host
          diReceiverPort:                       # (External Host) Port of OPTIC Data Lake Ingestion Service
          diAdminPort:                          # (External Host) Port of OPTIC Data Lake Administration Service
          diDataAccessPort:                     # (External Host) Port of OPTIC Data Lake Data Access Service
          diPulsarSslPort:                      # (External Host) Port of OPTIC Data Lake Message Bus SSL service
          diPulsarWebPort:                      # (External Host) Port of OPTIC Data Lake Message Bus Web service
      queryService:
        #deploy: true                           # Uncomment this parameter to deploy query service.
        
  prometheus:
    deployPrometheusConfig: true      #true = deploy scraping rules (ServiceMonitor, PodMonitor) and alerts (PrometheusRule)
    deployGrafanaConfig: true         #true = deploy configmaps defining the Grafana dashboards
 
 # Used to set label for fluent bit configuration. This parameter will be used by components to set label and value
 # The label name and value must match the definition under itom-fluentbit section further below
  fluentbit:
    fluentbitSelector:
      labelName: "deployment.microfocus.com/fluentbit-config"
      labelValue: "true"

  nodeSelector: {}

  #    If "persistence.enabled" is set to "true" then the PVCs(Persistent Volume Claim) will be automatically created when the chart is deployed. You do not need to fill the section.
  #    However, this requires that there are available PVs(Persistence Volume) to bind to. For opsbridge, 7 PVs are required if OBM is deployed in HA mode, else 6 PVs are required if OBM is deployed in non-HA mode.
  #    You must create the PVs before deploying the chart to make auto PVC assignments possible.
  #
  #    If "persistence.enabled" is set to "false" then you must create the PVCs as well as the PVs
  #    before deploying the chart and fill the section below.

  # Define persistent storage (needed only if Manual PVC is selected e.g. persistence.enabled: false):
  #    dataVolumeClaim is a Persistent Volume Claim (PVC) for storing data files.
  #    dbVolumeClaim is a PVC for storing database files.
  #    configVolumeClaim is a PVC for storing configuration files.
  #    logVolumeClaim is a PVC for storing log files.
  #    pvc-omi-0, pvc-omi-1 and omi-artemis-pvc are PVCs for storing OBM files

  persistence:
    enabled: true            # set to "true" to enable auto-PVC creation (requires available PVs) # for manually created PVC add the 5 PVC described above. You must pass the same value during install and upgrade. For example, if you had set “persistence: enabled: true” during install, for upgrade also set "persistence: enabled: true”.
    accessMode:              # Access Mode to be used in PVC created automatically by the chart

    # Specify the size of each of the PVCs, these values are used when persistence.enabled=true. Created PV size must match with the the PVC size given below.
    # For More Info on Sizing/Resizing of PVC, Please refer to the documentation.
    configVolumeSize: 10Gi
    dataVolumeSize: 10Gi
    dbVolumeSize: 10Gi
    logVolumeSize: 10Gi

    # Used to inject storageClasses into PVC creation.
    # User can change any/all to match the actual storageClasses used in their environment.
    storageClasses:
      # All 4 OPSB PVCs are created using "default-rwx", omi-artemis PVC is created using "default-rwo".
      default-rwx:                    # set it to "cdf-nfs" if using NFSProvisioner capability for PV creation
      default-rwo:                    # set it to "cdf-nfs" if using NFSProvisioner capability for PV creation

  docker:
    # The values below are default and already filled in to use internal docker repository that comes with CDF.
    # You only need to change the values when using external docker registry.
    registry: localhost:5000
    orgName: hpeswitom
    imagePullSecret: ""
    imagePullPolicy: IfNotPresent

    # The user/group IDs (UID/GID) for runtime deployment, and ownership of persistent storage.
    # if User 1999 is already in use by some other application then UID/fsGroup needs to be changed to different user.If UID/fsGroup is changed then same user should be used to setup NFS storage.
    # UID and GID must be the same
  securityContext:
    user: 1999
    fsGroup: 1999

  #k8s provider for cloud can be aws/azure, default is cdf
  cluster:
    k8sProvider: cdf

# Vertica Server details. When 'embedded' is set to "false", fill the parameters below for as per external External Vertica setup.
# Set embedded to "true" for Embedded Vertica (EVALUATION PURPOSE ONLY) and change the following four parameters given below (‘host’, ’rwuser’, ‘port’ and ‘rouser’) as described within their comments
  vertica:
    embedded: false
    host: itom-di-vertica-svc                                   # FQDN of Vertica Server. Incase of Vertica cluster with 3 nodes, enter comma separated list of FQDN of all the 3 nodes. In case of embedded Vertica, this parameter MUST be set to 'itom-di-vertica-svc'.
    rwuser: dbadmin                                             # DB User with READ and WRITE Permissions. In case of embedded Vertica, this parameter MUST be set to 'dbadmin'.
    rouser: dbadmin                                             # DB User with READ ONLY Permission. In case of embedded Vertica, this parameter MUST be set to 'dbadmin'.
    port: 5444                                                  # External vertica port for both TLS enabled and non-TLS. In case of embedded Vertica, this parameter MUST be set to '5444'.
    db: itomdb                                                  #  vertica database name
    rwuserkey: ITOMDI_DBA_PASSWORD_KEY                          # [DO NOT CHANGE] Password key for 'rwuser'
    rouserkey: ITOMDI_RO_USER_PASSWORD_KEY                      # [DO NOT CHANGE] Password key for 'rouser'
    tlsEnabled: true                                            # If set to "true", MUST provide Vertica Server Certificate during 'helm install' command for TLS Connection. Refer document for Installation command.
    resourcepoolname:                                           # Provide externally created and managed resource pool name IF the externally managed resource pool name is NOT the same as the default pool name ELSE can be left blank. Provide for external vertica, leave blank when vertica.embedded is set to true.

  # [DO NOT CHANGE] ConfigMap where Certificates are added.
  tlsTruststore: opsb-ca-certificate

  # Relational Database Details.(Postgres/Oracle).
  database:
    autoCreateDatabases: false
    databaseInitScript: opsb-database-init
    admin: postgres
    # If set to "true", Internal Postgres is used for database (For EVALUATION PURPOSE ONLY).
    internal: false     # The default is "false" to use external database, when set to false, fill the external database details in this section.

    # need to overwrite true from ucmdbserver
    createDb: false

    ## Parameters below are for external database only and MUST be provided when 'internal' is set to "false". You can skip this section ONLY if 'internal' is set to "true".
    ## You MUST also set the DB parameters at the later part of this file in this case. Fill the "idm:" , "autopass:", "obm:" and "bvd:" sections to set Database Username, Database Name for all components.

    # Default is "postgresql" to use External Postgres, change to "oracle" to use External Oracle.
    type: postgresql
    # DB Server Hostname. Required for Postgres and Oracle.
    host:
    # DB Port. Required for Postgres and Oracle.
    port:
    # When tlsEnabled is set to true, MUST provide database server certificate during installation.Refer Documentation for Installation Command. To Disable TLS, Change tlsEnabled to "false"
    tlsEnabled: true
    # [DO NOT CHANGE] ConfigMap where Certificates are added.
    tlsTruststore: opsb-ca-certificate

    # Opsbridge supports connection to Oracle DB by any of 2 ways -> 1) Connection String and 2) Oracle SID.
    # Please provide Any of the below given input to connect to DB.
    # Oracle Connection String. If Connection String is provided then 'oracleSid' is not used.[Used for Oracle only and need not be set if Postgres is used]
    oracleConnectionString:
    # Oracle SID [Used for Oracle only and need not be set if Postgres is used]
    oracleSid:
    #[DO NOT CHANGE] Name of the configmap where Oracle wallet is stored
    oracleWalletName: opsb-oracle-wallet

  apiClient:
    # [DO NOT CHANGE] this is used to inject certificates for Optic DL Message Bus client authentication
    authorizedClientCAs: api-client-ca-certificates

  # Agent Metric Collection settings
  # isAgentMetricCollectorEnabled: controls Agent Metric Collection functionality. 
  # Note: Node resolver, Data Broker and other sub-components will be started as dormant pods even if 'isAgentMetricCollectorEnabled' is set to false
  # autoStartAgentMetricCollector: auto-starts Agent Metric Collection on startup
  isAgentMetricCollectorEnabled: false
  autoStartAgentMetricCollector: true

  amc:
    # The location of the OBM server to which the Agent Metric Collector registers itself and from which Operations Agent nodes list is retrieved
    # OBM server can be external (located outside the cluster) or internal (located inside the cluster), which means external OBM can be classic or a containerized OBM running in a different cluster.
    externalOBM: false
    # Set to true if using containerized OBM running in a different cluster. If set to true, provide the external access hostname and port of the cluster in which the containerized OBM is running.
    # If set to false, provide the obm hostname and port of the classic OBM.
    containerizedOBM: true    
    # FQDN of OBM
    # If OBM is distributed 1GW, 1DPS, or if there's a load balancer mention in any one of the gateway or loadbalancer.
    # This parameter must be set when externalOBM: true
    # Ignored when externalOBM: false
    obmHostname:
    # The OBM server port used by components to access OBM and RTSM.If OBM is configured to be accessed as http, set this parameter to 80
    # Ignored when externalOBM: false
    port: 443
    # The OBM username used by components to access OBM and RTSM. Provide the OBM username having permissions to upload content packs
    # Ignored when externalOBM: false
    user: admin
    # The OBM user password key for user having content pack upload permissions
    # Ignored when externalOBM: false
    obmUserPassword: OBM_USER_PASSWORD_KEY  # [DO NOT CHANGE]
    # The protocol used by components to access OBM and RTSM. If OBM is configured to be accessed http, set this parameter to http.
    # Ignored when externalOBM: false
    rtsmProtocol: https
    # The username used by components to access OBM's RTSM. Provide the 'Agent Metric Collector integration user' that you had created
    # Ignored when externalOBM: false
    rtsmUsername:
    # Externally accessible port on the cluster used by external OBM to communicate with the Data Broker component of Agent Metric Collector
    # Ignored when externalOBM: false
    dataBrokerNodePort: 1383
    # The BBC port used by the OBM server for incoming connections. The Agent Metric Collector uses this port to communicate with OBM. The default port used by OBM is 383, therefore this setting should only be changed in case the default BBC port has been changed on the OBM server.
    # Ignored when externalOBM: false
    serverPort: 383
    # The Agent Metric Collector can connect to this many Operations Agent nodes in parallel during metric collection.
    # Use one of 5, 10, 20, 25. Note that higher parallel connections would consume more CPU and Memory resources than lower parallel connections.
    numOfParallelCollections: 25
    numOfParallelHistoryCollections: 10
    # Autoconfigure Job  
    autoconfigure:
      # Set to true to update if target , credential and collection manager object already exist
      UpdateConfigurations: true
      # Retry interval in minutes to check certificate exchange between OBM and Databroker
      CertCheckRetryInterval: 2
      # Number of retry attempt to check certificate exchange between OBM and Databroker before exiting the autoconfigure job
      CertCheckRetryCount: 60
      # Minutes to wait for the schema installation status check
      WaitForSchemaInstallation: 3
      # Number of retry attempts to be made for creating or updating the configuration on failure
      CreateOrUpdateRetryCount: 3
      # Retry interval in minutes to check endpoint availability
      EndpointRetryInterval: 1
      # Number of retry attempt to check endpoint availability
      EndpointRetryCount: 10
    # Provide the list of values for AMC collection configuration deployment
    customTqls: []

  # Pre-hook secret which will be used for validating checks by the pre-install/pre-upgrade hooks.
  preHookSecret: itom-opsb-db-connection-validator-secret

  # [DO NOT CHANGE] Kubernetes Secret created by "gen_secrets" script.
  initSecrets: opsbridge-suite-secret
  secretStorageName: opsbridge-suite-secret
  secretStorage: k8s

  # NGINX Ingress controller custom server certificates
  # key and certificate should start with -----BEGIN CERTIFICATE----- or -----BEGIN RSA PRIVATE KEY-----
  tls:
    cert: ""
    key: ""

  # [DO NOT CHANGE] Default Installation would be of Medium size. To change Deployment size, Please refer the Document.
  deployment:
    size: medium

  # [DO NOT CHANGE] Used by OPTIC Data Lake
  messageBus: pulsar

  #[DO NOT CHANGE] Used to enable dbvalidator
  validateDbConnection: true

  #[DO NOT CHANGE] Used to enable certificate validator
  validateCertificate: true

  # [DO NOT CHANGE] Used for OPTIC Data Lake
  clusterRoleCreate: false
  
  # [DO NOT CHANGE] Used for Suite Version
  applicationVersion: 24.2

  # [DO NOT CHANGE] Global image references.
  stunnel:
    image: itom-stunnel
    imageTag: 11.14.0-0029
  vaultRenew:
    image: kubernetes-vault-renew
    imageTag: 0.21.0-0046
  vaultInit:
    image: kubernetes-vault-init
    imageTag: 0.21.0-0046
  toolsBase:
    image: itom-tools-base
    imageTag: 1.6.0-0029
  itomRedis:
    image: itom-redis
    imageTag: 11.13.24

  # Optic DL related parameter
  di:
    # Name of the tenant that will be used while forming Vertica DB schema name
    tenant: "provider"
    # Deployment name that will be used while forming Vertica DB schema name
    deployment: "default"
    pulsar:
      # Name of the tenant provisioned in Pulsar
      tenant: "public"
      namespace: "default"
      client:
        # Name of the secret that holds the Pulsar client key and certificate that has tenant admin permission
        #tenantAdminSecret: "opsb-pulsar-tenant-admin-client-cert"
    externalDNS:
      enabled: false                           # Boolean value to enable automated route entry in DNS. Set this to true to enable access to services outside K8S cluster.
    externalAccessHost:
      #String Value for Optic DL Message Bus proxy DNS name. For optimized streaming Vertica needs to connect to Optic DL message bus. The hostname is the hostname under which Vertica DB can reach OPTIC DL's pulsar proxy.
      pulsar:
      #The hostname is the hostname under which Vertica DB can reach OPTIC DL's administration.
      administration:
      #The hostname is the hostname under which Vertica DB can reach OPTIC DL's dataAccess.
      dataAccess:
      #The hostname is the hostname under which Vertica DB can reach OPTIC DL's receiver.
      receiver:

  # [DO NOT CHANGE] Contains IDM parameters used across all the service charts.
  idm:
    hpssoInitStrSecretName: HPSSO_INIT_STRING_KEY
    transportUser: transport_admin
    transportUserKey: idm_transport_admin_password
    integrationUser: integration_admin
    integrationUserKey: idm_integration_admin_password
    serviceName: itom-idm-svc
    internalPort: 18443
    # Idm tenant name
    tenant: Provider
    orgAware: false
    additionalSeededOrg:
      name:
      displayName:

    # [DO NOT CHANGE] ServiceUrl will be used to validate the IDM Tokens
    idmServiceUrl: "https://itom-idm-svc:18443/idm-service"

  #[DO NOT CHANGE]
  # All boost related default values
  # All AppRole related values should  be same as initSecrets, it's not really an approle it's actually secret name
  # The above applies to all values associated to roles on the boost section.
  boost:
    event:
      eventingEnabled: false
    datasource:
      passwordAppRole: "opsbridge-suite-secret"
    rabbitmq:
      passwordAppRole: "opsbridge-suite-secret"
    encrypt:
      passwordAppRole: "opsbridge-suite-secret"
    tls:
      keystorePassAppRole: "opsbridge-suite-secret"
      truststorePassAppRole: "opsbridge-suite-secret"
    auth:
      csrfCookieKey: XSRF-TOKEN
      csrfHeaderKey: X-XSRF-TOKEN
      appTransportPasswordAppRole: "opsbridge-suite-secret"
      cgroAdminAppRole: "opsbridge-suite-secret"
      appTransportUsername: integration_admin
      appTransportPasswordVaultKey: idm_integration_admin_password
    proxy:
      passwordAppRole: "opsbridge-suite-secret"
    client:
      csa:
        passwordAppRole: "opsbridge-suite-secret"
      idm:
        port: 18443 
        host: itom-idm-svc
        idmTransportPasswordAppRole: "opsbridge-suite-secret"
        idmTransportUser: transport_admin
        idmTransportPasswordVaultKey: idm_transport_admin_password

  # [DO NOT CHANGE] All UD related configurations 
  # isUDCollectionEnabled: controls UD driven functionality for Hyperscale Observability.
  isUDCollectionEnabled: true

  # [DO NOT CHANGE] Deploys and Configures containerized Operations Agent to be used with Hyperscale Observability Capability
  deployOperationsAgent: true
  
  # [DO NOT CHANGE] Required for AWS discovery collector used with Hyperscale Observability Capability
  cms:
    deployGateway: true
    externalOBM: false
    udProtocol: https
    #Provide External UCMDB Server details. Ignore if externalOBM is set to false. 
    #Please provide same values as part of ucmdbprobe and cmsgateway sections below
    udHostname: 
    port: 8443
    udUsername:
    secrets:
      UISysadmin: ucmdb_uisysadmin_password          #Incase global.cms.externalOBM is set to true, set global.cms.secrets.UISysadmin to UD_USER_PASSWORD
      admin: idm_admin_admin_password
      sysadmin: sys_admin_password
    timeZone: ""
  # [To decide whether we want to enable or disable Restrict Upgrade feature]
  enableRestrictUpgrade: true
  
  agentlessMonitoring:
    enableSitescope: false                     #enableSitescope must be set to true to connect to the containerized SiteScope provider directly (without EDGE), when MCC on SAAS.
    
  monitoringService:
    isStaticThresholdingEnabled: true               #Allows Static thresholds to be available in monitoringService
    isDynamicThresholdingEnabled: true              #Allows Dynamic/Baseline thresholds to be available in monitoringService 
    enableAutomaticViewDeployment: true              # [DO NOT CHANGE] Allows automatic deployment of UCMDB Views in Performance Dashboard for monitoring service based collectors
    enableAutomaticCPDeployment: true                #[DO NOT CHANGE] Allows automatic deployment of OBM Content Packs  for monitoring service based collectors
    embeddedStaticThresholding: false                # [DO NOT CHANGE] Required for Threshold Collectors to be deployed with Hyperscale Observability collectors
    enableAwsMonitor: true                           # enableAwsMonitor must be set to true to start AWS Collector pods and configure AWS Collectors in Hyperscale Observability
    enableGcpMonitor: true                           # enableGcpMonitor must be set to true to start GCP Collector pods and configure GCP Collectors in Hyperscale Observability
    enableAzureMonitor: true                         # enableAzureMonitor must be set to true to start Azure Collector pods and configure Azure Collectors in Hyperscale Observability
    enableKubernetesMonitor: true                    # enableKubernetesMonitor must be set to true to start Kubernetes Collector pods and configure Kubernetes Collectors in Hyperscale Observability
    enablePrometheusMonitor: true                    # enablePrometheusMonitor must be set to true to start Prometheus Collector pods and configure Prometheus Collectors in Hyperscale Observability
    enableVMwareMonitor: true                      # enableVMwareMonitor must be set to true to start VMware Collector pods and configure VMware Collectors in Hyperscale Observability
    virtualizationCollector:
      enableMetricCollection: true                  # enableMetricCollection must be set to true to start VMware Metric Collector pods. Set this to false to disable VMware Metric Collection 
      enableEventCollection: true                   # enableEventCollection must be set to true to start VMware Event Collector pods. Set this to false to disable VMware Event Collection 

    jobFetcher:
      image: itom-monitoring-job-fetcher
      imageTag: 24.2-114
    resultProcessor:
      image: itom-monitoring-result-processor
      imageTag: 24.2-60

  # [DO NOT CHANGE] Set to false to fix "OCTCR19U2143045 24.1 to 24.2 Upgrade: BVD pods are recreating" 
  autoEnableProbeTBA: false
  enableInternalTokenRefresh: false

#parameter where all database certificates are injected. Pass the certificates as --set-file "caCertificates.vertica-ca\.crt"=<vertica certificate file> --set-file "caCertificates.postgres\.crt"=<relational database certificate file>
caCertificates: {}

#parameters where the external CA Signed Certificate and Key for Optic DL Message Bus are injected. Pass the certificate and key as --set-file "messagebus.externalCa.tls.cert"=<certificate file> --set-file "messagebus.externalCa.tls.key"=<key file>
messagebus:
  externalCa:
    tls:
      cert: ""
      key: ""

#Secrets Password must be provided in Base64 encoded format.
secrets:
  #Admin Password for IDM admin user. This password will be used to log into IDM UI.
  idm_opsbridge_admin_password:

  #Verica DBA and RO User passwords
  ITOMDI_DBA_PASSWORD_KEY:
  ITOMDI_RO_USER_PASSWORD_KEY:

  #Postgres/Oracle Db Passwords for different users
  AUTOPASS_DB_USER_PASSWORD_KEY:
  BVD_DB_USER_PASSWORD_KEY:
  AEC_DB_USER_PASSWORD_KEY:
  CM_DB_PASSWD_KEY:
  IDM_DB_USER_PASSWORD_KEY:
  MA_DB_USER_PASSWORD_KEY:
  SNF_DB_USER_PASSWORD_KEY:
  OBM_MGMT_DB_USER_PASSWORD_KEY:
  OBM_EVENT_DB_USER_PASSWORD_KEY:
  RTSM_DB_USER_PASSWORD_KEY:
  BTCD_DB_PASSWD_KEY:

  #Postgres/Oracle Db Admin Password. This password is used when auto-create option for databases is enabled.
  DB_ADMIN_PASSWORD_KEY:

  #Password for Monitoring User.This password will be used to log into Grafana UI
  ITOMDI_MONITOR_PWD_KEY:

  #Password for External OBMs RTSM user.The username will be provided in Helm values.yaml under global.amc.rtsmUsername
  OBM_RTSM_PASSWORD:

  #Password for External OBMs user with Content Packs upload permissions. The username will be provided in Helm values.yaml under global.monitoringService.obmUsername  
  OBM_USER_PASSWORD_KEY:

  #UCMDB Master Key.This key is not related to any configured UCMDB schema or database password. Its value can be anything as it is used for encryption.You can provide any value for this encryption key but its length must be of 32 chars only
  ucmdb_master_key:

  #System Administrator Password used for OBM JMX and UCMDB sysadmin user
  sys_admin_password:

  #Password for External Universal Discovery user.The username will be provided in Helm values.yaml under global.cms.udUsername
  UD_USER_PASSWORD:

  #Password for smtpServer user. Used for report scheduling with pdf-print tool.
  schedule_mail_password_key:

  # IDM Integration User password of the providing deployment when shared optic DL is used.
  OPTIC_DATALAKE_INTEGRATION_PASSWORD:

  #Admin Password for IDM SAAS admin user.
  idm_opsbridge_saas_admin_password:
  
  #Password for fluentbit reciever like elasticsearch 
  fluentbit_receiver_password_key:

# Below section deals with specific DB parameters for IDM, Autopass, BVD and OBM.
# This section needs to match the user/DB details created during Prepare Relational Database step.Since this is not creating specified username and DB Name but referring to already created relational DB details.
# This section has to be edited only in case of external postgres or external oracle. In case of internal postgres, leave the section to defaults.
# For Oracle, only 'user' is required. 'dbName' is not used. The 'user' is used for the Oracle schema in case of Idm, Autopass, Bvd and UCMDB. OBM uses 'eventUser' and 'mgmtUser' as schemas . Provide DIFFERENT users for each of IDM, Autopass, BVD, UCMDB and OBM.
# For Postgres, both 'user' and 'dbName' are required. Provide the different users for all the postgres databases i.e for IDM, Autopass, BVD, UCMDB and OBM. Since different user is the owner for all databases, different password keys is used for all Users.
# For OBM, eventDbName and mgmtDbName must have the same password as they use the same user in case of postgres. For Oracle, schema specified under eventUser and mgmtUser must have the same password.
# The password keys are set while running "gen_secrets" script and refers to password for the DB User. You don't need to edit 'userPasswordKey'.

# All IDM related parameters which are specific only to IDM are given below
idm:
  deployment:
    database:
      dbName: idm                                       # e.g. idm
      user: idm
      userPasswordKey: IDM_DB_USER_PASSWORD_KEY         # [DO NOT CHANGE]

# All Autopass related parameters which are specific only to Autopass are given below
# schema: If Databases are created using script, do not change the value else schema name should be what has been set in "PreparePostgreSQL" step for autopassdb.
autopass:
  deployment:
    database:
      dbName: autopass
      schema: autopassschema                           # Used Only for Postgres
      user: autopass
      userPasswordKey: AUTOPASS_DB_USER_PASSWORD_KEY          # [DO NOT CHANGE]

# All BVD related parameters which are specific only to BVD are given below
bvd:
  deployment:
    database:
      dbName: bvd                                        # e.g. bvd
      user: bvd
      userPasswordKey: BVD_DB_USER_PASSWORD_KEY           # [DO NOT CHANGE]
      oracleEncWallet:                             # Oracle Wallet File for TLS Verification
  params:
    suite:
      release: 24.2                            # Suite Version to be displayed in UI
  smtpServer:
    host:
    port:
    security:                               # Specify the value as  eg: TLS or STARTTLS
    user:
    from:
    passwordKey: schedule_mail_password_key # [DO NOT CHANGE] Password key for mailProxy user

aec:
  deployment:
    database:
      dbName: aec
      user: aec
      userPasswordKey: AEC_DB_USER_PASSWORD_KEY

#Parameter where oracle wallet zip's base64 file is passed
oracleWallet:

# All OBM related parameters which are specific only to OBM are given below
obm:
  params:
    haEnabled: true                                     # OBM HA is enabled by default. To Disable HA, Update the parameter to false.
    managementPacks:                                    # All Management Pack marked as true will be deployed during Installation
      ADMP: false
      AMCMP: false
      AmazonWebServicesMP: false
      BPMContent: false
      DiagnosticsMP: false
      Example_Policy_Templates: false
      ExchangeMP: false
      GoogleCloudMP: false
      HANAMP: false
      IISMP: false
      InfraMP: true
      JbsMP: false
      MSSQLMP: false
      Microsoft365MP: false
      MicrosoftAzureMP: false
      MySQLMP: false
      OraMP: false
      PostgreSQLMP: false
      RealUserMonitorMP: false
      SAPMP: false
      SiteScopeMetricStreamingMP: false
      SiteScope_Event_Integration: false
      VMWareInfraMP: false
      WbsMP: false
      WebLogicMP: false
  deployment:
    database:
      postgresCrlCheckEnabled: true
    mgmtDatabase:
      dbName: obm_mgmt                                  # Used for Postgres
      user: obm_mgmt
      userPasswordKey: OBM_MGMT_DB_USER_PASSWORD_KEY         # [DO NOT CHANGE]
    eventDatabase:
      dbName: obm_event                                 # Used for Postgres
      user: obm_event
      userPasswordKey: OBM_EVENT_DB_USER_PASSWORD_KEY         # [DO NOT CHANGE]

ucmdbserver:
  resources:
    requests:
      cpu: 1               # ONLY USE: Small: 0.5, Medium: 1, Large: 2
      memory: 5Gi          # ONLY USE: Small: 4,   Medium: 5, Large: 10
    limits:
      cpu: 2               # ONLY USE: Small: 2,   Medium: 2, Large: 4
      memory: 7Gi          # ONLY USE: Small: 5,   Medium: 7, Large: 11
  deployment:
    replicaCount: 2        # To disable HA, change this value to 1.
    database:
      dbName: rtsm
      user: rtsm
      schema: public
      userPasswordKey: RTSM_DB_USER_PASSWORD_KEY           # [DO NOT CHANGE]
    multiTenant: true                                      # [DO NOT CHANGE]
    lwssoEnabled: false                                    # [DO NOT CHANGE]
    upgradeCp: true                                        # [DO NOT CHANGE]
    nestedClassEnabled: false                              # [DO NOT CHANGE]
    mamGuiDefaultBundles: "Service_Health,SLM"             # [DO NOT CHANGE]
    defaultUserPasswordPolicyEnabled: false                # [DO NOT CHANGE]
    embeddedMode: true                                     # [DO NOT CHANGE]
    instanceSize: S        # ONLY USE: Small: S,    Medium: S,    Large: M
    jvmXmsMemory: 1024     # ONLY USE: Small: 512,  Medium: 1024, Large: 2048
    jvmXmxMemory: 5120     # ONLY USE: Small: 3072, Medium: 5120, Large: 8192

# All itom-opsbridge-monitoring-admin related configurations
itomopsbridgemonitoringadmin:
  deployment:
    database:
      dbName: monitoringadmindb
      user: monitoringadminuser
      userPasswordKey: MA_DB_USER_PASSWORD_KEY               # [DO NOT CHANGE]
  config:
    featuregate:
      configmap: opsb-cm

itommonitoringsnf:
  deployment:
    database:
      dbName: monitoringsnfdb
      user: monitoringsnfuser
      userPasswordKey: SNF_DB_USER_PASSWORD_KEY               # [DO NOT CHANGE]


# [DO NOT CHANGE]
ingressController:
  ports:
    receiver: 32765

# Before provisioning the OPTIC Data Lake Message Bus component, zookeeper disk size, bookkeeper journal size, bookkeeper ledgers size parameters should be set based on the deployment size so that PV to PVC binding happens correctly
itomdipulsar:
  proxy:
    config:
      useExternalCASignedCerts: false  #Optional, Toggle to indicate using external CA signed server cert on or off.
      caSignedServerCertSecretName: opsb-messagebus-external-ca-cert   #Optional, Name of the secret which contains the public & private key files. This will be honored only if proxy.config.useExternalCASignedCerts parameter is true
  pulsar:
    additionalSuperUserRoles: itom-co-collector,itom-cms-dataflowprobe  # [DO NOT CHANGE] This allows agent metric collector to authenticate and send metrics into OPTIC DL Message Bus, itom-cms-dataflowprobe - This allows Dataflow probe to authenticate and send topology data into Optic DL Message Bus
  bookkeeper:
    volumes:
      ledgers:
        name: ledgers
        size: 20Gi
      journal:
        name: journal
        size: 4Gi
  zookeeper:
    volumes:
      data:
        name: zookeeper-data
        size: 4Gi

# [DO NOT CHANGE]
itomdireceiver:
  direceiver:
    config:
      receiver:
        headerFieldnameForTopic: x-monitoredcollection
        headerFieldnameForTopicKey: x-monitoredsystem

itomdiadministration:
  diadmin:
    config:
      admin:
        adminUserOrg: Provider

itomdiquery:
  itomdiquery:
    dbuser:
    password: ITOMDI_QUERY_DB_PASSWORD_KEY

# [DO NOT CHANGE]
itom-ingress-controller:
  backwardsCompatServiceName: false
  defaultBackendRequired: false
  nginx:
    tls:
      cert: ""
      key: ""
    secretName: nginx-default-secret  # Secret which contains the ingress controller certificate and key
    configuration:
      keep-alive-requests: "10000" # Setting to work around wrong http/2 GOAWAY handing in chrome. See https://trac.nginx.org/nginx/ticket/2155 for more details.
      keep-alive: "210"            # Setting to work around wrong http/2 GOAWAY handing in chrome. See for https://trac.nginx.org/nginx/ticket/2155 for more details.
      proxy-body-size: 400m        # Setting to increase the max body-size to 400mb from the default 20mb. This is required by Application Monitoring.

itom-di-udx-scheduler:
  scheduler:
    configData:
      scheduler:
        frameDuration: "00:02:00"

# [DO NOT CHANGE] This starts Internal Postgres with TLS enabled
# This starts Internal Postgres with TLS enabled
# To add multiple users for internal postgres set the parameter as users:<db1>:<db1passwordkey>&<db2>:<db2passwordkey>&<db3>:<db3passwordkey>
#Eg users: idm:IDM_DB_USER_PASSWORD_KEY&autopass:AUTOPASS_DB_USER_PASSWORD_KEY&bvd:BVD_DB_USER_PASSWORD_KEY&obm:OBM_DB_USER_PASSWORD_KEY&rtsm:RTSM_DB_USER_PASSWORD_KEY
postgres15:
  postgres:
    tlsEnabled: true
    dbaUsers: postgres:IDM_DB_USER_PASSWORD_KEY
    users: idm:IDM_DB_USER_PASSWORD_KEY&autopass:AUTOPASS_DB_USER_PASSWORD_KEY&bvd:BVD_DB_USER_PASSWORD_KEY&aec:AEC_DB_USER_PASSWORD_KEY&obm:OBM_MGMT_DB_USER_PASSWORD_KEY&obm_mgmt:OBM_MGMT_DB_USER_PASSWORD_KEY&obm_event:OBM_EVENT_DB_USER_PASSWORD_KEY&rtsm:RTSM_DB_USER_PASSWORD_KEY&monitoringadminuser:MA_DB_USER_PASSWORD_KEY&credentialmanageruser:CM_DB_PASSWD_KEY&monitoringsnfuser:SNF_DB_USER_PASSWORD_KEY&btcd:BTCD_DB_PASSWD_KEY

  deployment:
    suiteInitScripts: opsb-database-init

  # Added to overcome embedded Postgres issues with HSO
  resources:
    limits:
      cpu: "2000m"
      memory: "5120Mi"
    requests:
      cpu: "100m"
      memory: "1024Mi"


itomopsbridgedatabaseinit:
  databaseinit:
    userPasswordList: btcd:btcd:BTCD_DB_PASSWD_KEY&aec:aec:AEC_DB_USER_PASSWORD_KEY
    dbaUserPasswordList: postgres:IDM_DB_USER_PASSWORD_KEY
    configMapName: internal-database-init


# [DO NOT CHANGE] Increases the Memory Limit of Task Executor to 3 Gi
itomdipostload:
  resources:
    taskExecutor:
      limits:
        memory: 3Gi
  dipostload:
   config:
     postload:
       postResourcePool: "itom_di_postload_respool_provider_default"

# All Credential Manager related configurations
# Set below indicated three values only if external oracle is used with wallet based authentication.
# oracleWalletName: Oracle Wallet configmap name
credentialmanager:
  deployment:
    rbac:
      # ServiceAccount name
      serviceAccount: "credential-manager-sa"
    database:
      dbName: credentialmanager
      user: credentialmanageruser
      schema: credentialmanagerdbschema   
      userPasswordKey: CM_DB_PASSWD_KEY                     # [DO NOT CHANGE]
      oracleWalletName: 

# [DO NOT CHANGE] Enables Performance Troubleshooter in standalone mode
itomnomcosodataaccess:
  enableMultiTenancy: "false"
  deployment:
    idm:
      user: integration_admin
      passwordKey: idm_integration_admin_password

nomapiserver:
  deployment:
    idm:
      user: integration_admin
      passwordKey: idm_integration_admin_password                # [DO NOT CHANGE]
    zookeeper:
      user: integration_admin
      passwordKey: NOM_ZK_ADMIN_PASSWORD_VAULT_KEY               # [DO NOT CHANGE]
  nom:
    mixedMode: "none"
  nnmi:
    user: "none"
    passwordKey: "none"
    url: "none"
    failoverUrl: "none"
  na:
    url: "none"

nomxui:
  adminUser: integration_admin
  adminUserPasswordKey: idm_integration_admin_password            # [DO NOT CHANGE]
  perfTroubleshooting:
    datasource: "coso"
  skipNomConfig: true
  jobResources:
    limits:
      memory: "256Mi"

itomopsbridgedes:
  des:
    # enableEnrichment=true allows downtime enrichment to be processed for incoming metrics
    # enableEnrichment=false sets DES to work in passthrough mode
    enableEnrichment: true
    # enrichmentDenylistPrefix - comma separated list of ODL topics that should not undergo enrichment. These metrics will be passthrough
    enrichmentDenylistPrefix:
    # enrichmentAllowlistPrefix - comma separated list of ODL topics that should undergo enrichment
    enrichmentAllowlistPrefix: opsb_agent,opsb_ad,opsb_jboss,opsb_sap,opsb_hana,opsb_exch,opsb_mssql,opsb_internal_mssql,opsb_ora,opsb_internal_ora,opsb_synthetic
    # enableCIEnrichment=true allows CI enrichment to be processed for incoming metrics
    # enableCIEnrichment=false DES passthrough CI enrichment
    enableCIEnrichment: true
    # receiverNodePort is the port to be used to expose the DES outside the container
    receiverNodePort: 30010
  cicache:
    # When external OBM is configured, Data enrichment Service will collect CIs from RTSM at fixed interval provided with ciCollectionIntervalMin. Value should be provided in minutes.
    ciCollectionIntervalMin: "60"
  deployment:
    config:
      replica: 1

# Configuration parameters for ucmdb probe integration with external OBM Server (UCMDB Server)
ucmdbprobe:
  secret: opsbridge-suite-secret                   # [DO NOT CHANGE]
  deployment:
    ucmdbProbes: itom,vcenter
    type: embedded                                 # Set value to standalone if global.cms.externalOBM is set to true
    ucmdbServer:
      hostName: itom-ucmdb-writersvc               #Provide external UCMDB Server hostname if global.cms.externalOBM is set to true
      port: 8443                                   #Provide external UCMDB Server port if global.cms.externalOBM is set to true
    database:            
      adminPasswordKey: ITOM_UCMDB_DB_PASSWD_KEY   # [DO NOT CHANGE]
    secrets:
      probePgRoot: ITOM_UCMDB_DB_PASSWD_KEY        # [DO NOT CHANGE]
      probePg: ucmdb_probe_pg_probe_password       # [DO NOT CHANGE]
  #UCMDB Probe certificate validation level (0,1,2).
  #0:Full validation
  #1:Full validation without revocation check (default)
  #2:Basic validation
    probeSSLFullValidation: 1   

# Configuration parameters required by cms gateway for external OBM (UCMDB Server) integration
cmsgateway:
  deployment:
    ucmdb:
      protocol: "https"
      host: itom-ucmdb-svc      #Provide external UCMDB Server hostname if global.cms.externalOBM is set to true
      port: 8443                #Provide external UCMDB Server port if global.cms.externalOBM is set to true
      userName: UISysadmin      # External OBM RTSM Username if global.cms.externalOBM is set to true
    database:
      adminPasswordKey: ITOM_UCMDB_DB_PASSWD_KEY   # [DO NOT CHANGE]

#Monitoring Service Data Broker specific values
itomopsbridgedatabroker:
  deployment:
    config:
    # Use OBM Data Collector settings below only if Data Collector URL is different from the OBM User URL
    # obmDataCollectorProtocol is the protocol to be used by the Data Broker to connect to the OBM server
      obmDataCollectorProtocol: https
    # obmDataCollectorHostname is the hostname to be used by the Data Broker to connect to the OBM server
      obmDataCollectorHostname:
    # obmDataCollectorPort is the port to be used by the Data Broker to connect to the OBM server
      obmDataCollectorPort: 383

# Configuration parameters required by the Anomaly Detection capability
itom-oba-config:
  deployment:
    oba:
      protocol: https
      host:                                   # Operations Bridge Analytics application server host
      configParameterServicePort: 9090

# Tune the performance of itom-nom-metric-transformation pod using the below configurations.
nommetricstransform:
  deployment:
    database:
      dbName: btcd                              # OK to change for EXTERNAL Postgres
      user: btcd                                # OK to change for EXTERNAL Postgres
      userPasswordKey: BTCD_DB_PASSWD_KEY       # [DO NOT CHANGE]
  baseline:
    healthCheckTopic: "cloud_collection_statistics"
    reinitConfigs: "false"

# DO NOT CHANGE: the prometheus-cert-exporter needs these settings to add and monitor SSL endpoints
prometheus-cert-exporter:
  global:
    rbac:
      clusterRoleCreate: false
  certExporter:
    configMapName: "cert-exporter-cm"

# Adding resource limit for consumer Upgrade restriction scenario

resources:
  limits:
    cpu: "50m"
    memory: "50Mi"
  requests:
    cpu: "5m"
    memory: "5Mi"

# This has two sections-
# deployment: is to add correct label in fluent-bit. Any changes in this section must be accompanied by changes in global.fluentbit section as well.
# logging: is to configure the receiver service that will handle processed data. Uncomment this section only if you intend to use a receiver app with fluentbit
itom-fluentbit:
  deployment:
    instances:              # [DO NOT CHANGE] The parameters below instances should not be changed
    - name: opsb
      configSelector:
        labelName: "deployment.microfocus.com/fluentbit-config"
        labelValue: "true"

  logging:
    output:
      receiver:
#        type:             # Set to "elasticsearch" to receive processed data from fluentbit
#        url:              # The URL of the Elasticsearch host in the format <https>://<elasticsearch-host>:<elasticsearch-port>
#        user:             # The name of the user account to login to Elasticsearch
#        tlsVerify:        # Set to "Off" to skip certificate validation. Defaults to "On"
        passwordKey: "fluentbit_receiver_password_key"         # [DO NOT CHANGE] The password of the user account to login to Elasticsearch
        elasticSearch:
          index: itom-fluentbit    # [DO NOT CHANGE] This index name should be created in elasticsearch

