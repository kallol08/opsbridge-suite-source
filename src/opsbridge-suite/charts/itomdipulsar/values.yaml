#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

global:
  topologySpreadConstraintsDefaults:
    enabled: true
    template:
      maxSkew: 1
      topologyKey: "topology.kubernetes.io/zone"
      whenUnsatisfiable: ScheduleAnyway
  expose:
    ipConfig:
      ipFamilyPolicy: PreferDualStack
      ipFamilies:
  cluster:
    k8sProvider: cdf
  rbac:
    serviceAccountCreate: true
    roleCreate: true
    clusterRoleCreate: false

  nodeSelector: {}

  # blank by default (instead of not present) to avoid errors
  apiClient:
    authorizedClientCAs:

  # If deployPrometheusConfig is true, CDF Monitoring Framework is assumed to be installed and will use it for metric storage
  prometheus:
    deployPrometheusConfig: true
    prometheusSelector:
      prometheus_config: "1"
    scrapeCertSecretName: "itom-di-prometheus-scrape-cert"
    scrapeCaConfigmapName: "monitoring-ca-certificates"

  persistence:
    enabled: false
    dataVolumeClaim:
    logVolumeClaim:

  localPersistence:
    enabled: true

  docker:
    # set the global.docker.registry and orgName to your internal Docker registry/org
    registry: localhost:5000
    orgName: hpeswitom
    imagePullSecret: ""
    imagePullPolicy: IfNotPresent

  securityContext:
    user: "1999"
    fsGroup: "1999"

  # Global image references for vault
  vaultRenew:
    #registry:
    #orgName:
    image: kubernetes-vault-renew
    imageTag: 0.10.0-0019 
  vaultInit:
    #registry:
    #orgName:
    image: kubernetes-vault-init
    imageTag: 0.10.0-0019 
  toolsBase:
    image: itom-tools-base
    imageTag: 1.1.0-0018

  externalAccessHost: ""

  #Will be used as a suffix for schema names
  di:
    tenant: "provider"
    deployment: "default"
    avroSchemaEvolutionEnabled: false
    externalAccessHost:
      pulsar:
    cloud:
      externalDNS:
        enabled: false
      externalAccessHost:
        pulsar:
    proxy:
      nodePorts:
        https:
        pulsarssl:
    prometheus:
      alerts:
        enabled: true

deployment:
  rbac:
    serviceAccount: ""
    proxyServiceAccount: ""
    brokerServiceAccount: ""
    multiAZServiceAccount: "itomdipulsar-multiaz-sa"
    multiAZClusterRole: "itomdipulsar-multiaz-cr"
    multiAZRole: "itomdipulsar-multiaz"
    commonClusterRole: "itomdipulsar-cr"
  topologySpreadConstraints:
    - maxSkew: 1
      topologyKey: "topology.kubernetes.io/zone"
      whenUnsatisfiable: ScheduleAnyway

nodeSelector: {}

# Flag to control whether to run initialize job
initialize: true

###
### K8S Settings
###

###
### Global Settings
###

## Pulsar Metadata Prefix
##
## By default, pulsar stores all the metadata at root path.
## You can configure to have a prefix (e.g. "/my-pulsar-cluster").
## If you do so, all the pulsar and bookkeeper metadata will
## be stored under the provided path
metadataPrefix: ""


## AntiAffinity
##
## Flag to enable and disable `AntiAffinity` for all components.
## This is a global setting that is applied to all components.
## If you need to disable AntiAffinity for a component, you can set
## the `affinity.anti_affinity` settings to `false` for that component.
affinity:
  anti_affinity: true
  # Set the anti affinity type. Valid values:
  # requiredDuringSchedulingIgnoredDuringExecution  - rules must be met for pod to be scheduled (hard) requires at least one node per replica
  # preferredDuringSchedulingIgnoredDuringExecution - scheduler will try to enforce but not guaranteed
  type: preferredDuringSchedulingIgnoredDuringExecution

## Components
##
## Control what components of Apache Pulsar to deploy for the cluster
components:
  # zookeeper
  zookeeper: true
  # bookkeeper
  bookkeeper: true
  # bookkeeper - autorecovery
  autorecovery: true
  # broker
  broker: true
  # functions
  functions: true
  # proxy
  proxy: true
  # bastion
  bastion: true
  # pulsar manager
  pulsar_manager: false
  # pulsar sql
  sql_worker: false
  # kop
  kop: false
  # pulsar detector
  pulsar_detector: false

#####################
# Image definitions
#####################
pulsar:
  image: itom-pulsar-core
  imageTag: 2.10.4-2.12.1-11
  pullPolicy: Always
  additionalSuperUserRoles:

## TLS
## templates/tls-certs.yaml
##
## The chart is using cert-manager for provisioning TLS certs for
## brokers and proxies.
tls:
  enabled: true
  # common settings for generating certs
  common:
    # 90d
    duration: 2160h
    # 15d
    renewBefore: 360h
    organization:
      - pulsar
    keySize: 4096
    keyAlgorithm: rsa
    keyEncoding: pkcs8
  # settings for generating certs for proxy
  proxy:
    enabled: true
    cert_name: tls-proxy
  # settings for generating certs for proxy
  pulsar_detector:
    enabled: false
    cert_name: tls-pulsar-detector
  # settings for generating certs for broker
  broker:
    enabled: true
    cert_name: tls-broker
  # settings for generating certs for bookies
  bookie:
    enabled: true
    cert_name: tls-bookie
  # settings for generating certs for zookeeper
  zookeeper:
    enabled: true
    cert_name: tls-zookeeper
  # settings for generating certs for recovery
  autorecovery:
    cert_name: tls-recovery
  # settings for generating certs for bastion
  bastion:
    cert_name: tls-bastion
  pulsar_manager:
    enabled: false
    cert_name: tls-pulsar-manager

# Enable or disable broker authentication and authorization.
auth:
  authentication:
    enabled: true
    provider: "tlscert"
    jwt:
      # Enable JWT authentication
      # If the token is generated by a secret key, set the usingSecretKey as true.
      # If the token is generated by a private key, set the usingSecretKey as false.
      usingSecretKey: false
  authorization:
    enabled: false
  superUsers:
    # broker to broker communication
    broker: "broker-admin"
    # proxy to broker communication
    proxy: "proxy-admin"
    # pulsar-admin client to broker/proxy communication
    client: "admin"
    # pulsar-manager to broker/proxy communication
    pulsar_manager: "pulsar-manager-admin"
  # Enable vault based authentication
  vault:
    enabled: false
######################################################################
# External dependencies
######################################################################

## cert-manager
## templates/tls-cert-issuer.yaml
##
## Cert manager is used for automatically provisioning TLS certificates
## for components within a Pulsar cluster
certs:
  internal_issuer:
    enabled: false
    component: internal-cert-issuer
    type: selfsigning
  public_issuer:
    enabled: false
    component: public-cert-issuer
    type: acme
  issuers:
    selfsigning:
    acme:
      # You must replace this email address with your own.
      # Let's Encrypt will use this to contact you about expiring
      # certificates, and issues related to your account.
      email: contact@example.local
      # change this to production endpoint once you successfully test it
      # server: https://acme-v02.api.letsencrypt.org/directory
      server: https://acme-staging-v02.api.letsencrypt.org/directory
      solver: clouddns
      solvers:
        clouddns:
          # TODO: add a link about how to configure this section
          project: "[YOUR GCP PROJECT ID]"
          serviceAccountSecretRef:
            name: "[NAME OF SECRET]"
            key: "[KEY OF SECRET]"
        # route53:
        #   region: "[ROUTE53 REGION]"
        #   secretAccessKeySecretRef:
        #     name: "[NAME OF SECRET]"
        #     key: "[KEY OF SECRET]"
        #   role: "[ASSUME A ROLE]"
  lets_encrypt:
    ca_ref:
      secretName: "[SECRET STORES lets encrypt CA]"
      keyName: "[KEY IN THE SECRET STORES let encrypt CA]"

## External DNS
## templates/external-dns.yaml
## templates/external-dns-rbac.yaml
##
## External DNS is used for synchronizing exposed Ingresses with DNS providers
external_dns:
  enabled: false
  component: external-dns
  policy: upsert-only
  registry: txt
  owner_id: pulsar
  domain_filter: pulsar.example.local
  provider: google
  providers:
    google:
      # project: external-dns-test
      project: "[GOOGLE PROJECT ID]"
    aws:
      zoneType: public
  serviceAcct:
    annotations: {}
  securityContext: {}

## Domain requested from External DNS
domain:
  enabled: false
  suffix: test.pulsar.example.local

## Ingresses for exposing Pulsar services
ingress:
  ## templates/proxy-service-ingress.yaml
  ##
  ## Ingresses for exposing pulsar service publicly
  proxy:
    enabled: false
    tls:
      enabled: true
    type: LoadBalancer
    annotations: {}
    extraSpec: {}
  ## templates/broker-service-ingress.yaml
  ##
  ## Ingresses for exposing pulsar service publicly
  broker:
    enabled: false
    type: LoadBalancer
    annotations: {}
    extraSpec: {}
  ## templates/control-center-ingress.yaml
  ##
  ## Ingresses for exposing monitoring/management services publicly
  controller:
    enabled: false
    rbac: true
    component: nginx-ingress-controller
    replicaCount: 1
    # cloud.google.com/gke-nodepool: default-pool
    tolerations: []
    gracePeriod: 300
    annotations: {}
    ports:
      http: 80
      https: 443
    # flag whether to terminate the tls at the loadbalancer level
    tls:
      termination: false
  control_center:
    enabled: true
    component: control-center
    endpoints:
      grafana: true
      prometheus: false
      alertmanager: false
    # Set external domain of the load balancer of ingress controller
    # external_domain: your.external.control.center.domain
    # external_domain_scheme: https://
    tls:
      enabled: false
    annotations: {}


######################################################################
# Below are settings for each component
######################################################################

## Common properties applied to pulsar components
common:
  extraInitContainers: {}

## To Deploy Application with cluster wide permissions
itomdipulsar:
  configureDataPlacementPolicy: true
  alerts:
    enableBacklogQuotaAlert: false
  

## Enable tenant-mgmt tool for execution. This is applicable only for SAAS scenario
enableTenantMgmtTool: false

## Pulsar: Zookeeper cluster
## templates/zookeeper-statefulset.yaml
##
zookeeper:
  # use a component name that matches your grafana configuration
  # so the metrics are correctly rendered in grafana dashboard
  component: zookeeper
  # the number of zookeeper servers to run. it should be an odd number larger than or equal to 3.
  replicaCount: 3
  ports:
    metrics: 8000
    client: 2181
    clientTls: 2281
    follower: 2888
    leaderElection: 3888
    # cloud.google.com/gke-nodepool: default-pool
  probe:
    liveness:
      enabled: false
      initialDelaySeconds: 10
      periodSeconds: 30
      timeoutSeconds: 1
      failureThreshold: 10
      successThreshold: 1
    readiness:
      enabled: false
      initialDelaySeconds: 10
      periodSeconds: 30
      timeoutSeconds: 1
      failureThreshold: 10
      successThreshold: 1
    startup:
      enabled: false
      initialDelaySeconds: 10
      periodSeconds: 30
      timeoutSeconds: 1
      failureThreshold: 30
      successThreshold: 1
  affinity:
    anti_affinity: true
    # Set the anti affinity type. Valid values:
    # requiredDuringSchedulingIgnoredDuringExecution  - rules must be met for pod to be scheduled (hard) requires at least one node per replica
    # preferredDuringSchedulingIgnoredDuringExecution - scheduler will try to enforce but not guaranteed
    type: preferredDuringSchedulingIgnoredDuringExecution

  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "8000"
    pod.boostport.com/vault-init-container: install
  securityContext: {}
  tolerations: []
  gracePeriod: 30
  resources:
    requests:
      memory: 512Mi
      cpu: 0.5
    limits:
      memory: 4Gi
      cpu: 2
  volumes:
    # use a persistent volume or emptyDir
    persistence: true
    # Add a flag here for backward compatibility. Ideally we should
    # use two disks for production workloads. This flag might be
    # removed in the future releases to stick to two-disks mode.
    useSeparateDiskForTxlog: false
    data:
      name: zookeeper-data
      size: 20Gi
      local_storage: true
      storageClassName: "fast-disks"
      ## If the storage class is left undefined when using persistence
      ## the default storage class for the cluster will be used.
      ##
      # storageClass:
      #   type: pd-ssd
      #   fsType: xfs
      #   provisioner: kubernetes.io/gce-pd
      #   allowVolumeExpansion: false
      #   volumeBindingMode: Immediate
      #   reclaimPolicy: Retain
      #   allowedTopologies:
      #   mountOptions:
      #   extraParameters:
      #     iopsPerGB: "50"
    dataLog:
      name: datalog
      size: 10Gi
      local_storage: true
      # storageClassName: ""
      ## If the storage class is left undefined when using persistence
      ## the default storage class for the cluster will be used.
      ##
      # storageClass:
      #   type: pd-ssd
      #   fsType: xfs
      #   provisioner: kubernetes.io/gce-pd
      #   allowVolumeExpansion: false
      #   volumeBindingMode: Immediate
      #   reclaimPolicy: Retain
      #   allowedTopologies:
      #   mountOptions:
      #   extraParameters:
      #     iopsPerGB: "50"
  extraInitContainers: {}
  ## Zookeeper configmap
  ## templates/zookeeper-configmap.yaml
  ##
  # The initial myid used for generating myid for each zookeeper pod.
  initialMyId: 0
  peerType: "participant"
  # reconfig settings
  reconfig:
    enabled: false
    # The zookeeper servers to observe/join
    zkServers: []
  # Automtically Roll Deployments when configmap is changed
  autoRollDeployment: true
  configData:
    PULSAR_MEM: >
      -Xms64m -Xmx384m -Djdk.tls.maxCertificateChainLength=20
    PULSAR_GC: >
      -XX:+UseG1GC
      -XX:MaxGCPauseMillis=10
      -Djute.maxbuffer=10485760
      -XX:+ParallelRefProcEnabled
      -XX:+UnlockExperimentalVMOptions
      -XX:+DoEscapeAnalysis
      -XX:+DisableExplicitGC
      -XX:+PerfDisableSharedMem
      -Dzookeeper.forceSync=no
      -XX:+ExitOnOutOfMemoryError
      
    sslQuorum: "true"
  ## Zookeeper service
  ## templates/zookeeper-service.yaml
  ##
  service:
    annotations:
      service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
  ## Zookeeper PodDisruptionBudget
  ## templates/zookeeper-pdb.yaml
  ##
  pdb:
    usePolicy: true
    maxUnavailable: 1

## Pulsar: Bookkeeper cluster
## templates/bookkeeper-statefulset.yaml
##
bookkeeper:
  # use a component name that matches your grafana configuration
  # so the metrics are correctly rendered in grafana dashboard
  component: bookkeeper
  ## BookKeeper Cluster Initialize
  ## templates/bookkeeper-cluster-initialize.yaml
  metadata:
    ## Set the resources used for running `bin/bookkeeper shell initnewcluster`
    ##
    resources:
      requests:
        memory: 2Gi
        cpu: 2
      limits:
        memory: 4Gi
        cpu: 4
  replicaCount: 3
  ports:
    http: 8000
    bookie: 3181
    # cloud.google.com/gke-nodepool: default-pool
  probe:
    liveness:
      enabled: false
      initialDelaySeconds: 10
      periodSeconds: 30
      timeoutSeconds: 1
      failureThreshold: 60
      successThreshold: 1
    readiness:
      enabled: false
      initialDelaySeconds: 10
      periodSeconds: 30
      timeoutSeconds: 1
      failureThreshold: 60
      successThreshold: 1
    startup:
      enabled: false
      initialDelaySeconds: 60
      periodSeconds: 30
      timeoutSeconds: 1
      failureThreshold: 30
      successThreshold: 1
  affinity:
    anti_affinity: true
    # Set the anti affinity type. Valid values:
    # requiredDuringSchedulingIgnoredDuringExecution  - rules must be met for pod to be scheduled (hard) requires at least one node per replica
    # preferredDuringSchedulingIgnoredDuringExecution - scheduler will try to enforce but not guaranteed
    type: preferredDuringSchedulingIgnoredDuringExecution

  annotations: {}
  securityContext: {}
  tolerations: []
  gracePeriod: 30
  resources:
    requests:
      memory: 2Gi
      cpu: 0.5
    limits:
      memory: 4Gi
      cpu: 4
  volumes:
    # use a persistent volume or emptyDir
    persistence: true
    journal:
      name: journal
      size: 20Gi
      local_storage: true
      storageClassName: "fast-disks"
      ## If the storage class is left undefined when using persistence
      ## the default storage class for the cluster will be used.
      ##
      # storageClass:
      #   type: pd-ssd
      #   fsType: xfs
      #   provisioner: kubernetes.io/gce-pd
      #   allowVolumeExpansion: false
      #   volumeBindingMode: Immediate
      #   reclaimPolicy: Retain
      #   allowedTopologies:
      #   mountOptions:
      #   extraParameters:
      #     iopsPerGB: "50"
    ledgers:
      name: ledgers
      size: 20Gi
      local_storage: true
      storageClassName: "fast-disks"
      ## If the storage class is left undefined when using persistence
      ## the default storage class for the cluster will be used.
      ##
      # storageClass:
        # type: pd-ssd
        # fsType: xfs
        # provisioner: kubernetes.io/gce-pd
        # allowVolumeExpansion: false
        # volumeBindingMode: Immediate
        # reclaimPolicy: Retain
        # allowedTopologies:
        # mountOptions:
        # extraParameters:
        #   iopsPerGB: "50"
  extraInitContainers: {}
  ## Bookkeeper configmap
  ## templates/bookkeeper-configmap.yaml
  ##
  # Automtically Roll Deployments when configmap is changed
  autoRollDeployment: true
  placementPolicy:
    rackAware: true
  configData:
    # `BOOKIE_MEM` is used for `bookie shell`
    BOOKIE_MEM: >
      -Xms2g
      -Xmx2g
      -XX:MaxDirectMemorySize=1g
    # we use `bin/pulsar` for starting bookie daemons
    PULSAR_MEM: >
      -Xms2g
      -Xmx2g
      -XX:MaxDirectMemorySize=1g
      -Djdk.tls.maxCertificateChainLength=20
    BOOKIE_GC: >
      -XX:+UseG1GC
      -XX:MaxGCPauseMillis=10
      -XX:+ParallelRefProcEnabled
      -XX:+UnlockExperimentalVMOptions
      -XX:+DoEscapeAnalysis
      -XX:ParallelGCThreads=4
      -XX:ConcGCThreads=4
      -XX:G1NewSizePercent=50
      -XX:+DisableExplicitGC
      -XX:-ResizePLAB
      -XX:+ExitOnOutOfMemoryError
      -XX:+PerfDisableSharedMem
      -XX:+PrintGCDetails
      -verbosegc

    journalSyncData: "false"
    journalMaxGroupWaitMSec: "3"
    # ledgerStorageClass: "org.apache.bookkeeper.bookie.SortedLedgerStorage"
    autoRecoveryDaemonEnabled: "false"
    openFileLimit: "100000"
    pageSize: "8192"
    minorCompactionMaxTimeMillis: "900000"
    majorCompactionMaxTimeMillis: "1800000"
    isForceGCAllowWhenNoSpace: "true"
    forceAllowCompaction: "true"
    managedLedgerDigestType: "DUMMY"
    journalMaxSizeMB: "2048"
    # journalMaxBackups: "0"
    # statsProviderClass: org.apache.bookkeeper.stats.prometheus.PrometheusMetricsProvider
    # useHostNameAsBookieID: "false"
    ensemblePlacementPolicy: org.apache.bookkeeper.client.RackawareEnsemblePlacementPolicy
    reppDnsResolverClass: org.apache.pulsar.zookeeper.ZkBookieRackAffinityMapping
    enforceMinNumRacksPerWriteQuorum: "false"
    minNumRacksPerWriteQuorum: "2"
    allowLoopback: "true"
    gcWaitTime: "60000"


  ## Bookkeeper Service
  ## templates/bookkeeper-service.yaml
  ##
  service:
    annotations:
      publishNotReadyAddresses: "true"
  ## Bookkeeper PodDisruptionBudget
  ## templates/bookkeeper-pdb.yaml
  ##
  pdb:
    usePolicy: true
    maxUnavailable: 1

## Pulsar: Bookkeeper AutoRecovery
## templates/autorecovery-statefulset.yaml
##
autorecovery:
  # use a component name that matches your grafana configuration
  # so the metrics are correctly rendered in grafana dashboard
  component: autorecovery
  replicaCount: 1
  ports:
    http: 8000
    # cloud.google.com/gke-nodepool: default-pool
  affinity:
    anti_affinity: true
    # Set the anti affinity type. Valid values:
    # requiredDuringSchedulingIgnoredDuringExecution  - rules must be met for pod to be scheduled (hard) requires at least one node per replica
    # preferredDuringSchedulingIgnoredDuringExecution - scheduler will try to enforce but not guaranteed
    type: preferredDuringSchedulingIgnoredDuringExecution

  annotations: {}
  securityContext: {}
  # tolerations: []
  gracePeriod: 30
  resources:
    requests:
      memory: 512Mi
      cpu: 0.05
    limits:
      memory: 2Gi
      cpu: 0.5
  extraInitContainers: {}
  ## Bookkeeper auto-recovery configmap
  ## templates/autorecovery-configmap.yaml
  ##
  # Automtically Roll Deployments when configmap is changed
  autoRollDeployment: true
  configData:
    JMXDISABLE: "true"
    BOOKIE_MEM: >
      -Xms64m -Xmx64m

## Pulsar Zookeeper metadata. The metadata will be deployed as
## soon as the last zookeeper node is reachable. The deployment
## of other components that depends on zookeeper, such as the
## bookkeeper nodes, broker nodes, etc will only start to be
## deployed when the zookeeper cluster is ready and with the
## metadata deployed
pulsar_metadata:
  component: zookeeper-metadata
  ## set an existing configuration store
  # configurationStore:
  configurationStoreMetadataPrefix: ""

  ## optional, you can provide your own zookeeper metadata store for other components
  # to use this, you should explicit set components.zookeeper to false
  #
  # userProvidedZookeepers: "zk01.example.com:2181,zk02.example.com:2181"

  # set the cluster name. if empty or not specified,
  # it will use helm release name to generate a cluster name.
  clusterName: ""

## Pulsar: KoP Protocol Handler
kop:
  ports:
    plaintext: 9092
    ssl: 9093

## Pulsar: Broker cluster
## templates/broker-statefulset.yaml
##
broker:
  # use a component name that matches your grafana configuration
  # so the metrics are correctly rendered in grafana dashboard
  component: broker
  replicaCount: 3
  ports:
    http: 8080
    https: 8443
    pulsar: 6650
    pulsarssl: 6651
    # cloud.google.com/gke-nodepool: default-pool
  probe:
    liveness:
      enabled: true
      initialDelaySeconds: 30
      periodSeconds: 10
      timeoutSeconds: 5
      failureThreshold: 10
      successThreshold: 1
    readiness:
      enabled: true
      initialDelaySeconds: 30
      periodSeconds: 10
      timeoutSeconds: 5
      failureThreshold: 10
      successThreshold: 1
    startup:
      enabled: false
      initialDelaySeconds: 60
      periodSeconds: 10
      timeoutSeconds: 1
      failureThreshold: 30
      successThreshold: 1
  affinity:
    anti_affinity: true
    # Set the anti affinity type. Valid values:
    # requiredDuringSchedulingIgnoredDuringExecution  - rules must be met for pod to be scheduled (hard) requires at least one node per replica
    # preferredDuringSchedulingIgnoredDuringExecution - scheduler will try to enforce but not guaranteed
    type: preferredDuringSchedulingIgnoredDuringExecution

  annotations: {}
  tolerations: []
  securityContext: {}
  gracePeriod: 30
  # flag to advertise pod ip address
  advertisedPodIP: true
  resources:
    requests:
      memory: 2Gi
      cpu: 0.5
    limits:
      memory: 4Gi
      cpu: 4
  extraInitContainers: {}
  ## Broker configmap
  ## templates/broker-configmap.yaml
  ##
  # Automtically Roll Deployments when configmap is changed
  autoRollDeployment: true
  configData:
    PULSAR_MEM: >
      -Xms2g -Xmx2g -XX:MaxDirectMemorySize=1g -Djdk.tls.maxCertificateChainLength=20
    PULSAR_GC: >
      -XX:+UseG1GC
      -XX:MaxGCPauseMillis=10
      -XX:+UseContainerSupport
      -Dio.netty.leakDetectionLevel=disabled
      -Dio.netty.recycler.linkCapacity=1024
      -XX:+ParallelRefProcEnabled
      -XX:+UnlockExperimentalVMOptions
      -XX:+DoEscapeAnalysis
      -XX:ParallelGCThreads=4
      -XX:ConcGCThreads=4
      -XX:G1NewSizePercent=50
      -XX:+DisableExplicitGC
      -XX:-ResizePLAB
      -XX:+ExitOnOutOfMemoryError
      -XX:+PerfDisableSharedMem
    AWS_ACCESS_KEY_ID: "[YOUR AWS ACCESS KEY ID]"
    AWS_SECRET_ACCESS_KEY: "[YOUR SECRET]"
    # managedLedgerDefaultEnsembleSize: "2"
    # managedLedgerDefaultWriteQuorum: "2"
    # managedLedgerDefaultAckQuorum: "2"
    # ITOM DI Changes
    defaultRetentionTimeInMinutes: "15"
    defaultRetentionSizeInMB: "20"
    backlogQuotaDefaultRetentionPolicy: "producer_exception"
    ttlDurationDefaultInSeconds: "0"
    managedLedgerMinLedgerRolloverTimeMinutes: "1"
    managedLedgerMaxLedgerRolloverTimeMinutes: "2"
    managedLedgerCursorRolloverTimeInSeconds: "120"
    brokerDeduplicationEnabled: "true"
    brokerDeduplicationMaxNumberOfProducers: "10000"
    brokerDeduplicationEntriesInterval: "1000"
    brokerDeduplicationProducerInactivityTimeoutMinutes: "360"
    deduplicationEnabled: "false"
    allowAutoTopicCreation: "false"
    brokerDeleteInactiveTopicsEnabled: "false"
    exposePreciseBacklogInPrometheus: "true"
    metricsServletTimeoutMs: "120000"
    # brokerPublisherThrottlingMaxByteRate is applied globally: 0 means no limit, positive value applies a threshold.
    brokerPublisherThrottlingMaxByteRate: "0"
    supportedNamespaceBundleSplitAlgorithms: "range_equally_divide"
    # brokerServicePortTls: "6651"
    # webServicePortTls: "8443"
    authorizationEnabled: "false"
    # proxyRoles: "proxy-admin"
    tlsProtocols: "TLSv1.2,TLSv1.3"
    tlsProvider: "JDK"
    # functionsWorkerEnabled: "true"
    tlsCiphers: "TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_DHE_RSA_WITH_AES_256_GCM_SHA384,TLS_AES_128_GCM_SHA256,TLS_AES_256_GCM_SHA384"
    bookkeeperClientMinNumRacksPerWriteQuorum: "2"
    bookkeeperClientEnforceMinNumRacksPerWriteQuorum: "false"
    loadBalancerLoadSheddingStrategy: "org.apache.pulsar.broker.loadbalance.impl.ThresholdShedder"
    preciseTopicPublishRateLimiterEnable: "true"
    maxTenants: "0"
    maxNamespacesPerTenant: "0"
    maxTopicsPerNamespace: "0"
    systemTopicEnabled: "true"
    topicLevelPoliciesEnabled: "true"
  publicDefaultNamespaceSettings:
    # backlogQuotaTopicDefaultLimitSize is divided across topic partitions
    backlogQuotaTopicDefaultLimitSize: "20M"
    backlogQuotaDefaultRetentionPolicy: "producer_exception"
    

  ## Broker service
  ## templates/broker-service.yaml
  ##
  service:
    annotations: {}
  ## Broker PodDisruptionBudget
  ## templates/broker-pdb.yaml
  ##
  pdb:
    usePolicy: true
    maxUnavailable: 1
  ### Broker service account
  ## templates/broker-service-account.yaml
  service_account:
    annotations: {}
  offload:
    enabled: false
    managedLedgerOffloadDriver: aws-s3
    gcs:
      enabled: false
      gcsManagedLedgerOffloadRegion: "[YOUR REGION OF GCS]"
      gcsManagedLedgerOffloadBucket: "[YOUR BUCKET OF GCS]"
      gcsManagedLedgerOffloadMaxBlockSizeInBytes: "67108864"
      gcsManagedLedgerOffloadReadBufferSizeInBytes: "1048576"
    s3:
      enabled: false
      s3ManagedLedgerOffloadRegion: "[YOUR REGION OF S3]"
      s3ManagedLedgerOffloadBucket: "[YOUR BUCKET OF S3]"
      s3ManagedLedgerOffloadMaxBlockSizeInBytes: "67108864"
      s3ManagedLedgerOffloadReadBufferSizeInBytes: "1048576"

## Pulsar: Functions Worker
## templates/function-worker-configmap.yaml
##
functions:
  component: functions-worker
  enableCustomizerRuntime: true
  runtimeCustomizerClassName: "com.swgrp.itomdi.pulsar.functions.runtime.kubernetes.COSOKubernetesManifestCustomizer"
  pulsarExtraClasspath: "extraLibs"
  # Specify the namespace to run pulsar functions
  jobNamespace: ""
  configData:
    assignmentWriteMaxRetries: 60
    authenticationEnabled: true
    authenticationProviders:
    - org.apache.pulsar.broker.authentication.AuthenticationProviderTls
    #authorizationEnabled: false
    authorizationProvider: org.apache.pulsar.broker.authorization.PulsarAuthorizationProvider
    brokerClientTrustCertsFilePath: /var/run/secrets/boostport.com/issue_ca.crt
    clientAuthenticationParameters: tlsCertFile:/var/run/secrets/boostport.com/server.crt,tlsKeyFile:/var/run/secrets/boostport.com/server.key
    clientAuthenticationPlugin: org.apache.pulsar.client.impl.auth.AuthenticationTls
    clusterCoordinationTopicName: coordinate
    configurationStoreServers: localhost:2181
    # Connectors
    connectorsDirectory: ./connectors
    downloadDirectory: download/pulsar_functions
    failureCheckFreqMs: 30000
    functionAssignmentTopicName: "assignments"
    functionAuthProviderClassName: com.swgrp.itomdi.pulsar.functions.auth.COSOKubernetesTlsFunctionAuthProvider
    functionMetadataTopicName: metadata
    # kubernetes runtime
    functionRuntimeFactoryClassName: org.apache.pulsar.functions.runtime.kubernetes.KubernetesRuntimeFactory
    functionsDirectory: ./functions
    initialBrokerReconnectMaxRetries: 60
    instanceLivenessCheckFreqMs: 30000
    numHttpServerThreads: 8
    pulsarFunctionsNamespace: public/functions
    rebalanceCheckFreqSec: -1
    rescheduleTimeoutMs: 60000
    schedulerClassName: "org.apache.pulsar.functions.worker.scheduler.RoundRobinScheduler"
    tlsAllowInsecureConnection: false
    tlsCertRefreshCheckDurationSec: 300
    tlsCertificateFilePath: /var/run/secrets/boostport.com/server.crt
    tlsEnableHostnameVerification: false
    tlsEnabled: true
    tlsKeyFilePath: /var/run/secrets/boostport.com/server.key
    # tlsTrustCertsFilePath: /var/run/secrets/boostport.com/issue_ca.crt
    # Frequency how often worker performs compaction on function-topics
    topicCompactionFrequencySec: 1800
    useCompactedMetadataTopic: false
    validateConnectorConfig: false
    workerHostname: localhost
    workerId: standalone
    workerPort: 6750
    workerPortTls: 6751
    zooKeeperOperationTimeoutSeconds: 30
    zooKeeperSessionTimeoutMillis: 30000
  functionRuntimeFactoryConfigs:
    customLabels: null
    expectedMetricsCollectionInterval: '30'
    extraFunctionDependenciesDir: null
    imagePullPolicy: Always
    javaInstanceJarLocation: null
    k8Uri: null
    logDirectory: logs/
    percentMemoryPadding: 10
    pythonInstanceLocation: null
  kubernetesContainerFactory:
    expectedMetricsCollectionInterval: '30'

## Pulsar: pulsar detector
## templates/pulsar-detector-statefulset.yaml
##
pulsar_detector:
  component: pulsar-detector
  replicaCount: 1

  gracePeriod: 30
  port: 9000
  ## Proxy service
  ## templates/pulsar-detector-service.yaml
  ##
  service:
    spec:
      clusterIP: None
    annotations: {}

  ## Pulsar detector PodDisruptionBudget
  ## templates/pulsar-detector-pdb.yaml
  ##
  pdb:
    usePolicy: true
    maxUnavailable: 1
## Pulsar: Proxy Cluster
## templates/proxy-statefulset.yaml
##
proxy:
  # use a component name that matches your grafana configuration
  # so the metrics are correctly rendered in grafana dashboard
  component: proxy
  replicaCount: 2
    # cloud.google.com/gke-nodepool: default-pool
  probe:
    liveness:
      enabled: true
      initialDelaySeconds: 30
      periodSeconds: 10
      timeoutSeconds: 5
      failureThreshold: 10
      successThreshold: 1
    readiness:
      enabled: true
      initialDelaySeconds: 30
      periodSeconds: 10
      timeoutSeconds: 5
      failureThreshold: 10
      successThreshold: 1
    startup:
      enabled: false
      initialDelaySeconds: 60
      periodSeconds: 10
      timeoutSeconds: 1
      failureThreshold: 30
      successThreshold: 1
  affinity:
    anti_affinity: true
    # Set the anti affinity type. Valid values:
    # requiredDuringSchedulingIgnoredDuringExecution  - rules must be met for pod to be scheduled (hard) requires at least one node per replica
    # preferredDuringSchedulingIgnoredDuringExecution - scheduler will try to enforce but not guaranteed
    type: preferredDuringSchedulingIgnoredDuringExecution

  annotations: {}
  securityContext: {}
  tolerations: []
  gracePeriod: 30
  resources:
    requests:
      memory: 2Gi
      cpu: 0.5
    limits:
      memory: 4Gi
      cpu: 4
  config:
    useExternalCASignedCerts: false
    caSignedServerCertSecretName:
    enableSecurityAudit: false
    securityAuditVolumeClaim:
  extraInitContainers: {}
  ## Proxy configmap
  ## templates/proxy-configmap.yaml
  ##
  # Automtically Roll Deployments when configmap is changed
  autoRollDeployment: true
  # Config proxy to point to an existing broker clusters
  brokerServiceURL: ""
  brokerWebServiceURL: ""
  brokerServiceURLTLS: ""
  brokerWebServiceURLTLS: ""
  configData:
    PULSAR_MEM: >
      -Xms2g -Xmx2g -XX:MaxDirectMemorySize=1g -Djdk.tls.maxCertificateChainLength=20
    PULSAR_GC: >
      -XX:+UseG1GC
      -XX:MaxGCPauseMillis=10
      -Dio.netty.leakDetectionLevel=disabled
      -Dio.netty.recycler.linkCapacity=1024
      -XX:+ParallelRefProcEnabled
      -XX:+UnlockExperimentalVMOptions
      -XX:+DoEscapeAnalysis
      -XX:ParallelGCThreads=4
      -XX:ConcGCThreads=4
      -XX:G1NewSizePercent=50
      -XX:+DisableExplicitGC
      -XX:-ResizePLAB
      -XX:+ExitOnOutOfMemoryError
      -XX:+PerfDisableSharedMem
    allowAutoTopicCreation: "false"
    tlsProtocols: "TLSv1.2,TLSv1.3"
    tlsProvider: "JDK"
    tlsCiphers: "TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_DHE_RSA_WITH_AES_256_GCM_SHA384,TLS_AES_128_GCM_SHA256,TLS_AES_256_GCM_SHA384"
  
  ## Proxy service
  ## templates/proxy-service.yaml
  ##
  ports:
    http: 8080
    https: 8443
    pulsar: 6650
    pulsarssl: 6651
  nodePorts:
    http: 30011    
    pulsar: 30050   
  service:
    annotations: {}
    type: NodePort
    extraSpec: {}
  ## Proxy PodDisruptionBudget
  ## templates/proxy-pdb.yaml
  ##
  pdb:
    usePolicy: true
    maxUnavailable: 1

## Pulsar ToolSet
## templates/bastion-deployment.yaml
##
bastion:
  component: bastion
  useProxy: true
  replicaCount: 1
    # cloud.google.com/gke-nodepool: default-pool
  annotations: {}
  tolerations: []
  gracePeriod: 0
  resources:
    requests:
      memory: 512Mi
      cpu: 0.1
    limits:
      memory: 2Gi
      cpu: 0.5
  # resources:
  #   requests:
  #     memory: 256Mi
  #     cpu: 0.1
  ## Bastion configmap
  ## templates/bastion-configmap.yaml
  ##
  # Automtically Roll Deployments when configmap is changed
  autoRollDeployment: true
  configData:
    PULSAR_MEM: >
      -Xms128M
      -Xmx256M
      -XX:MaxDirectMemorySize=128M
      -Djdk.tls.maxCertificateChainLength=20
    authPlugin: "org.apache.pulsar.client.impl.auth.AuthenticationTls"
    # authParams: "tlsCertFile:/var/run/secrets/boostport.com/server.crt,tlsKeyFile:/var/run/secrets/boostport.com/server.key"
    # tlsTrustCertsFilePath: "/var/run/secrets/boostport.com/issue_ca.crt"
sources_upgrade:
   component: minio-connector-upgrade
sources_delete:
   component: minio-connector-delete

diadmin:
  config:
    pulsar:
      topicPartitionCount: "3"  #All pulsar message bus configuration parameters
brokerSetting:
  resources:
    requests:
      memory: 512Mi
      cpu: 0.1
    limits:
      memory: 2Gi
      cpu: 0.5
multiAz:
  resources:
    requests:
      memory: 512Mi
      cpu: 0.2
    limits:
      memory: 2Gi
      cpu: 0.5
minioHook:
  resources:
    requests:
      memory: 512Mi
      cpu: 0.1
    limits:
      memory: 2Gi
      cpu: 0.5
initContainers:
  pulsar_bookkeeper_verify_clusterid:
    resources:
      requests:
        memory: 512Mi
        cpu: 0.1
      limits:
          memory: 2Gi
          cpu: 0.5
  wait_bookkeeper_ready:
    resources:
      requests:
        memory: 512Mi
        cpu: 0.1
      limits:
          memory: 2Gi
          cpu: 0.5
  wait_broker_ready:
    resources:
      requests:
        memory: 512Mi
        cpu: 0.1
      limits:
        memory: 2Gi
        cpu: 0.5
  wait_cs_ready:
    resources:
      requests:
        memory: 512Mi
        cpu: 0.1
      limits:
        memory: 2Gi
        cpu: 0.5
